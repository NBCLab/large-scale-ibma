import argparse
import os.path as op
import gzip
import pickle

from nimare.dataset import Dataset

from lda import annotate_lda, _annotate_dset
from utils import (
    _exclude_outliers,
    _fetch_neuroquery_dset,
    _generate_counts,
    _cogat_vocabulary,
    _add_texts,
)


def _get_parser():
    parser = argparse.ArgumentParser(description="Run IBMA workflow")
    parser.add_argument(
        "--project_dir",
        dest="project_dir",
        required=True,
        help="Path to project directory",
    )
    parser.add_argument(
        "--n_cores",
        dest="n_cores",
        default=4,
        required=False,
        help="CPUs",
    )
    return parser


def main(project_dir, n_cores):
    project_dir = op.abspath(project_dir)
    n_cores = int(n_cores)

    data_dir = op.join(project_dir, "data")
    nq_dir = op.join(data_dir, "neuroquery")
    cogat_dir = op.join(data_dir, "cogat")
    images_dir = op.join(data_dir, "pubmed_images")
    result_dir = op.join(project_dir, "results", "pubmed_ibma")

    # At least dset_fn must exist. It is generated by pubmed_download.py
    dset_fn = op.join(result_dir, "pubmed_dataset-raw.pkl.gz")
    dset_clean_fn = op.join(result_dir, "pubmed_dataset-filtered.pkl.gz")
    dset_lda_clean_fn = op.join(result_dir, "pubmed-lda_dataset-filtered.pkl.gz")

    nq_lda_fn = op.join(nq_dir, "neuroquery_lda_model.pkl.gz")
    nq_lda_dset_fn = op.join(nq_dir, "neuroquery_lda_dataset.pkl.gz")
    nq_dset_text_fn = op.join(nq_dir, "neuroquery_with-texts_dataset.pkl.gz")

    # Load NeuroQuery dataset with texts
    if not op.isfile(nq_dset_text_fn):
        nq_dset = _fetch_neuroquery_dset()

        # Add texts to NeuroQuery dataset
        nq_corpus_fn = op.join(nq_dir, "neuroquery_corpus_small.csv")

        nq_dset = _add_texts(nq_dset, nq_corpus_fn)
        nq_dset.save(nq_dset_text_fn)
    else:
        nq_dset = Dataset.load(nq_dset_text_fn)

    # Get vocabulary from cognitive atlas concepts
    vocabulary = _cogat_vocabulary(cogat_dir)

    # Generate counts for Neuroquery dataset using the vocabulary from cogat concepts
    nq_counts_df = _generate_counts(
        nq_dset.texts,
        vocabulary=vocabulary,
        text_column="body",
        tfidf=False,
        max_df=len(nq_dset.ids) - 2,
        min_df=2,
    )

    if not op.isfile(nq_lda_fn):
        nq_lda_dset, model = annotate_lda(
            nq_dset,
            nq_counts_df,
            n_topics=100,
            max_iter=1000,
            n_cores=n_cores,
        )
        # model.save(nq_lda_fn)
        with gzip.GzipFile(nq_lda_fn, "wb") as file_object:
            pickle.dump(model, file_object)

        nq_lda_dset.save(nq_lda_dset_fn)
    else:
        model_file = gzip.open(nq_lda_fn, "rb")
        model = pickle.load(model_file)

    # LDA model on NeuroVault dataset
    # Load NeuroVault dataset with Pubmed IDs, images and texts
    dset = Dataset.load(dset_fn)
    dset.update_path(images_dir)

    # Exclude outliers from NeuroVault dataset
    if not op.isfile(dset_clean_fn):
        dset_clean = _exclude_outliers(dset)
        dset_clean.save(dset_clean_fn)
    else:
        dset_clean = Dataset.load(dset_clean_fn)

    # Generate counts for NeuroVault dataset using the vocabulary from cogat concepts
    nv_counts_df = _generate_counts(
        dset_clean.texts,
        vocabulary=vocabulary,
        text_column="body",
        tfidf=False,
        max_df=len(dset_clean.ids) - 2,
        min_df=2,
    )

    # Transform NeuroVault dataset counts using NQ LDA model
    doc_topic_weights = model.model.transform(nv_counts_df.values)

    # Annotate NeuroVault dataset with LDA model transformed weights
    dset_lda = _annotate_dset(dset_clean, model.model, nv_counts_df, doc_topic_weights)
    dset_lda.save(dset_lda_clean_fn)


def _main(argv=None):
    option = _get_parser().parse_args(argv)
    kwargs = vars(option)
    main(**kwargs)


if __name__ == "__main__":
    _main()
