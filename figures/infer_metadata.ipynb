{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as op\n",
    "\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "from nilearn._utils.niimg_conversions import _check_same_fov\n",
    "from nilearn.image import concat_imgs, resample_to_img\n",
    "from nimare.stats import pearson\n",
    "from nimare.utils import get_masker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate images within collection, assuming the are in the same space and have\n",
    "# the same number of subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _n_contrast(df):\n",
    "    return df.cognitive_contrast_cogatlas_id.unique().size\n",
    "\n",
    "\n",
    "def _n_definitions(df):\n",
    "    return df.contrast_definition.unique().size\n",
    "\n",
    "\n",
    "def _get_count_by_collection(data_df, groupby, label, count_func=None):\n",
    "    if count_func:\n",
    "        out_df = data_df.groupby(groupby).apply(count_func)\n",
    "    else:\n",
    "        out_df = data_df.groupby(groupby).size()\n",
    "\n",
    "    out_df = pd.DataFrame(out_df)\n",
    "    out_df = out_df.rename(columns={out_df.columns[0]: label})\n",
    "\n",
    "    return out_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIERARCHY_DICT = {\n",
    "    \"None / Other\": -float(\"inf\"),\n",
    "    \"Other\": -float(\"inf\"),\n",
    "    \"None\": -float(\"inf\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_corr_matrix(images, masker):\n",
    "    _resample_kwargs = {\"clip\": True, \"interpolation\": \"linear\"}\n",
    "    imgs = [\n",
    "        (\n",
    "            nib.load(img)\n",
    "            if _check_same_fov(nib.load(img), reference_masker=masker.mask_img)\n",
    "            else resample_to_img(nib.load(img), masker.mask_img, **_resample_kwargs)\n",
    "        )\n",
    "        for img in images\n",
    "    ]\n",
    "\n",
    "    img4d = concat_imgs(imgs, ensure_ndim=4)\n",
    "    data = masker.transform(img4d)\n",
    "\n",
    "    return [pearson(img_map, data) for img_map in list(data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _n_cogats(df):\n",
    "    return df.cognitive_paradigm_cogatlas_id.unique().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _infer_contrast(text):\n",
    "    # TODO: infer the contrast name\n",
    "    # Actually, use regex to match the text to a contrast name\n",
    "    # Implement fuzzy matching, to list the cogatlas contrast names and select the most similar\n",
    "    # Use tokenizer from scikit learn to tokenize the text and compare with the contrast names\n",
    "    return text\n",
    "\n",
    "\n",
    "def _select_relevant_contrast(contrast_names):\n",
    "    if len(contrast_names) == 1:\n",
    "        return contrast_names[0]\n",
    "    else:\n",
    "        # Select one based on the hierarchy using the contrast name\n",
    "        scores = []\n",
    "        for contrast_name in contrast_names:\n",
    "            if contrast_name in HIERARCHY_DICT:\n",
    "                score = HIERARCHY_DICT[contrast_name]\n",
    "            else:\n",
    "                # If contrast name is not in the hierarchy, infer the contrast name\n",
    "                inferred_name = _infer_contrast(contrast_name)\n",
    "                score = HIERARCHY_DICT[inferred_name]\n",
    "            scores.append(score)\n",
    "\n",
    "        # If multiple contrast names have the same score, select the first one\n",
    "        return contrast_names[scores.index(max(scores))]\n",
    "\n",
    "\n",
    "def _heuristic_selection(data_df, columns):\n",
    "    \"\"\"Heuristic to select the most relevant image from a collection of images.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_df : pandas.DataFrame\n",
    "        Dataframe containing the images to select from.\n",
    "    columns : list of str\n",
    "        List of columns to use for the heuristic sorted by priority.\n",
    "        This columns are use for the recursive selection of the image.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Dataframe containing the selected image.\n",
    "    \"\"\"\n",
    "    assert len(columns) > 0, \"At least one column is required\"\n",
    "\n",
    "    # Drop rows with NaN in the first column\n",
    "    sub_df = data_df.dropna(subset=[columns[0]])\n",
    "\n",
    "    if sub_df.shape[0] == 0:\n",
    "        # No contrasts name, check the contrast_definition if the next element\n",
    "        # from columns is available\n",
    "        if len(columns) > 1:\n",
    "            return _heuristic_selection(data_df, columns[1:])\n",
    "        else:\n",
    "            # No contrast definition. Select random image from the collection\n",
    "            return data_df.sample(1)\n",
    "\n",
    "    elif sub_df.shape[0] == 1:\n",
    "        # Only one contrast definition, select it\n",
    "        return sub_df.copy()\n",
    "\n",
    "    else:\n",
    "        # Multiple contrast definitions\n",
    "        # Get unique contrast names, and select one based on the hierarchy\n",
    "        contrast_names = sub_df.cognitive_contrast_cogatlas_name.unique()\n",
    "        contrast_selected = _select_relevant_contrast(contrast_names)\n",
    "\n",
    "        # Select the image with the selected contrast\n",
    "        image_selected = sub_df[sub_df.cognitive_contrast_cogatlas_name == contrast_selected]\n",
    "        if image_selected.shape[0] == 1:\n",
    "            # Only one image with the selected contrast\n",
    "            return image_selected.copy()\n",
    "        else:\n",
    "            if len(columns) > 1:\n",
    "                # Actually, if if columns[0] it need to look at the next column\n",
    "                return _heuristic_selection(image_selected, columns[1:])\n",
    "            else:\n",
    "                # Multiple images with the selected contrast, select one randomly.\n",
    "                return image_selected.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data\"\n",
    "nv_collections_images_df = pd.read_csv(op.join(data_dir, \"nv_collections_images.csv\"))\n",
    "\n",
    "nv_collections_images_df[\"cognitive_contrast_cogatlas_id\"] = nv_collections_images_df[\n",
    "    \"cognitive_contrast_cogatlas_id\"\n",
    "].replace({\"Other\": None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_cogats = (\n",
    "    nv_collections_images_df.groupby(\"collection_id\").apply(_n_cogats).sort_values(ascending=True)\n",
    ")\n",
    "img_per_coll = nv_collections_images_df.groupby(\"collection_id\").size().sort_values(ascending=True)\n",
    "\n",
    "# Get collections with only one unique cognitive atlas and multiple images\n",
    "coll_unique_cogat = unique_cogats[unique_cogats == 1]\n",
    "img_per_coll_unique_cogat = img_per_coll[img_per_coll.index.isin(coll_unique_cogat.index)]\n",
    "coll_unique_cogat_multi_images = img_per_coll_unique_cogat[img_per_coll_unique_cogat > 1]\n",
    "coll_unique_cogat_multi_images = pd.DataFrame(coll_unique_cogat_multi_images)\n",
    "coll_unique_cogat_multi_images = coll_unique_cogat_multi_images.rename(\n",
    "    columns={coll_unique_cogat_multi_images.columns[0]: \"n_images\"}\n",
    ")\n",
    "coll_unique_cogat_multi_images = coll_unique_cogat_multi_images.reset_index()\n",
    "coll_unique_cogat_multi_images[\"n_tasks\"] = 1\n",
    "\n",
    "# The collections in the following dataframe have only 1 image, so they\n",
    "# do not need to go through multiple images selection process.\n",
    "coll_one_image = img_per_coll[img_per_coll == 1]\n",
    "coll_one_image_df = nv_collections_images_df[\n",
    "    nv_collections_images_df.collection_id.isin(coll_one_image.index)\n",
    "]\n",
    "\n",
    "# What about the collections with multiple unique cognitive atlas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "nv_collections_contrast_df = nv_collections_images_df.dropna(\n",
    "    subset=[\"cognitive_contrast_cogatlas_id\"]\n",
    ")\n",
    "nv_collections_definition_df = nv_collections_images_df.dropna(\n",
    "    subset=[\"contrast_definition\"]\n",
    ")\n",
    "\n",
    "n_contrasts_by_coll = _get_count_by_collection(\n",
    "    nv_collections_contrast_df, \n",
    "    \"collection_id\", \n",
    "    \"n_contrasts\",\n",
    ")\n",
    "n_unique_contrasts_by_coll = _get_count_by_collection(\n",
    "    nv_collections_contrast_df, \n",
    "    \"collection_id\", \n",
    "    \"n_unique_contrasts\", \n",
    "    count_func=_n_contrast,\n",
    ")\n",
    "\n",
    "n_contrasts_def_by_coll =  _get_count_by_collection(\n",
    "    nv_collections_definition_df, \n",
    "    \"collection_id\", \n",
    "    \"n_contrasts_def\",\n",
    ")\n",
    "n_unique_contrasts_def_by_coll = _get_count_by_collection(\n",
    "    nv_collections_definition_df, \n",
    "    \"collection_id\", \n",
    "    \"n_unique_contrasts_def\", \n",
    "    count_func=_n_definitions,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "coll_unique_cogat_multi_images = (\n",
    "    coll_unique_cogat_multi_images.merge(n_contrasts_by_coll, how=\"left\", on=\"collection_id\")\n",
    "    .merge(n_unique_contrasts_by_coll, how=\"left\", on=\"collection_id\")\n",
    "    .merge(n_contrasts_def_by_coll, how=\"left\", on=\"collection_id\")\n",
    "    .merge(n_unique_contrasts_def_by_coll, how=\"left\", on=\"collection_id\")\n",
    ")\n",
    "\n",
    "for column in [\"n_contrasts\", \"n_unique_contrasts\", \"n_contrasts_def\", \"n_unique_contrasts_def\"]:\n",
    "    coll_unique_cogat_multi_images[column] = coll_unique_cogat_multi_images[column].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "coll_unique_cogat_multi_images.to_csv(op.join(data_dir, \"nv_colls_unique_cogat_multi_images.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "collections = coll_unique_cogat_multi_images.index.to_list()\n",
    "\n",
    "image_selected_lst = []\n",
    "for collection in collections:\n",
    "    sub_df = nv_collections_images_df[nv_collections_images_df.collection_id == collection]\n",
    "\n",
    "    image_selected = _heuristic_selection(\n",
    "        sub_df,\n",
    "        [\n",
    "            \"cognitive_contrast_cogatlas_name\",\n",
    "            \"contrast_definition\",\n",
    "        ],\n",
    "    )\n",
    "    image_selected_lst.append(image_selected)\n",
    "# Look at the concept links to contrast\n",
    "\n",
    "image_selected_df = pd.concat(image_selected_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pmid</th>\n",
       "      <th>pmcid</th>\n",
       "      <th>doi</th>\n",
       "      <th>secondary_doi</th>\n",
       "      <th>collection_id</th>\n",
       "      <th>collection_name</th>\n",
       "      <th>source</th>\n",
       "      <th>image_name</th>\n",
       "      <th>map_type</th>\n",
       "      <th>image_file</th>\n",
       "      <th>collection_id.1</th>\n",
       "      <th>image_id</th>\n",
       "      <th>number_of_subjects</th>\n",
       "      <th>cognitive_paradigm_cogatlas_id</th>\n",
       "      <th>cognitive_contrast_cogatlas_id</th>\n",
       "      <th>contrast_definition</th>\n",
       "      <th>cognitive_paradigm_cogatlas_name</th>\n",
       "      <th>cognitive_contrast_cogatlas_name</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3317</th>\n",
       "      <td>38066069</td>\n",
       "      <td>10709616.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13520</td>\n",
       "      <td>Experience sampling reveals the role that cove...</td>\n",
       "      <td>pubmed</td>\n",
       "      <td>Non-target</td>\n",
       "      <td>Z</td>\n",
       "      <td>images/13520/nontarget_group.nii.gz</td>\n",
       "      <td>13520</td>\n",
       "      <td>798383</td>\n",
       "      <td>57</td>\n",
       "      <td>trm_4da86cfe8cf1b</td>\n",
       "      <td>Other</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sustained attention to response task</td>\n",
       "      <td>Other</td>\n",
       "      <td>13520-798383_nontarget_group.nii.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1218</th>\n",
       "      <td>29069521</td>\n",
       "      <td>5716095.0</td>\n",
       "      <td>10.1093/scan/nsx125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3952</td>\n",
       "      <td>Love flows downstream: mothers’ and children’s...</td>\n",
       "      <td>neurovault</td>\n",
       "      <td>children's brain self-family contrast negative...</td>\n",
       "      <td>Z</td>\n",
       "      <td>images/3952/zstat8.nii.gz</td>\n",
       "      <td>3952</td>\n",
       "      <td>65066</td>\n",
       "      <td>22</td>\n",
       "      <td>trm_4da890594742a</td>\n",
       "      <td>cnt_553a6ea7469af</td>\n",
       "      <td>mothers' attention to self pain &gt; their child ...</td>\n",
       "      <td>emotional regulation task</td>\n",
       "      <td>all</td>\n",
       "      <td>3952-65066_zstat8.nii.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1163</th>\n",
       "      <td>29753107</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1016/j.neuroimage.2018.05.025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3814</td>\n",
       "      <td>The neural basis of free language choice in bi...</td>\n",
       "      <td>neurovault</td>\n",
       "      <td>Figure 2a</td>\n",
       "      <td>T</td>\n",
       "      <td>images/3814/spmT_0002.nii.gz</td>\n",
       "      <td>3814</td>\n",
       "      <td>64145</td>\n",
       "      <td>15</td>\n",
       "      <td>tsk_4a57abb949cfb</td>\n",
       "      <td>cnt_4df6a77053b3f</td>\n",
       "      <td>naming in English (L2) versus naming in German...</td>\n",
       "      <td>picture naming task</td>\n",
       "      <td>accuracy of participant minus average accuracy...</td>\n",
       "      <td>3814-64145_spmT_0002.nii.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1165</th>\n",
       "      <td>25062683</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1016/j.biopsych.2014.06.005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3655</td>\n",
       "      <td>Corticostriatal Control of Goal-Directed Actio...</td>\n",
       "      <td>neurovault</td>\n",
       "      <td>Valued actions simple effect in 18 healthy adults</td>\n",
       "      <td>T</td>\n",
       "      <td>images/3655/spmT_0002.nii.gz</td>\n",
       "      <td>3655</td>\n",
       "      <td>62601</td>\n",
       "      <td>18</td>\n",
       "      <td>trm_4f2414059baa8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Simple effect of valued actions delta function</td>\n",
       "      <td>instrumental learning task</td>\n",
       "      <td>None / Other</td>\n",
       "      <td>3655-62601_spmT_0002.nii.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>29437891</td>\n",
       "      <td>5858598.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3554</td>\n",
       "      <td>Abstract memory representations in the ventrom...</td>\n",
       "      <td>pubmed</td>\n",
       "      <td>Prototype model correlates</td>\n",
       "      <td>Z</td>\n",
       "      <td>images/3554/zstat1.nii.gz</td>\n",
       "      <td>3554</td>\n",
       "      <td>61824</td>\n",
       "      <td>29</td>\n",
       "      <td>trm_4f24112057e90</td>\n",
       "      <td>cnt_55a3034ca0943</td>\n",
       "      <td>prototype model predictor versus implicit base...</td>\n",
       "      <td>categorization task</td>\n",
       "      <td>percent correct</td>\n",
       "      <td>3554-61824_zstat1.nii.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2601</th>\n",
       "      <td>34147817</td>\n",
       "      <td>8220377.0</td>\n",
       "      <td>10.1016/j.nicl.2021.102723</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8993</td>\n",
       "      <td>Whole-brain functional correlates of memory fo...</td>\n",
       "      <td>neurovault</td>\n",
       "      <td>across group subsequentmemory scenes participa...</td>\n",
       "      <td>T</td>\n",
       "      <td>images/8993/across-group_subsequentmemory-scen...</td>\n",
       "      <td>8993</td>\n",
       "      <td>459316</td>\n",
       "      <td>60</td>\n",
       "      <td>tsk_mFS3uwUMAhXxe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>across-groups</td>\n",
       "      <td>Memory encoding task</td>\n",
       "      <td>None / Other</td>\n",
       "      <td>8993-459316_across-group_subsequentmemory-scen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3198</th>\n",
       "      <td>37713673</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1162/jocn_a_02055</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13693</td>\n",
       "      <td>Neural Mechanisms Underlying Trust to Friends,...</td>\n",
       "      <td>neurovault</td>\n",
       "      <td>Positive T-test Whole brain Regression with Ge...</td>\n",
       "      <td>T</td>\n",
       "      <td>images/13693/spmT_0001_6.nii.gz</td>\n",
       "      <td>13693</td>\n",
       "      <td>795325</td>\n",
       "      <td>92</td>\n",
       "      <td>tsk_Ncknr0soiM4IV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>social decision-making task</td>\n",
       "      <td>None / Other</td>\n",
       "      <td>13693-795325_spmT_0001_6.nii.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>27989844</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1016/j.neuroimage.2016.10.041</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1848</td>\n",
       "      <td>In need of constraint: Understanding the role ...</td>\n",
       "      <td>neurovault</td>\n",
       "      <td>Seed_i9_Perseverance_NegativeContrast_zstat</td>\n",
       "      <td>Z</td>\n",
       "      <td>images/1848/zstat13_4.nii.gz</td>\n",
       "      <td>1848</td>\n",
       "      <td>28822</td>\n",
       "      <td>204</td>\n",
       "      <td>trm_56a91a92043bc</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UPPS-P Impulsivity Scale</td>\n",
       "      <td>None / Other</td>\n",
       "      <td>1848-28822_zstat13_4.nii.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831</th>\n",
       "      <td>28957344</td>\n",
       "      <td>5619738.0</td>\n",
       "      <td>10.1371/journal.pone.0185152</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2853</td>\n",
       "      <td>Functional hemispheric asymmetries during the ...</td>\n",
       "      <td>neurovault</td>\n",
       "      <td>Planning lateralization right hand S</td>\n",
       "      <td>T</td>\n",
       "      <td>images/2853/spmT_0003_1.nii.gz</td>\n",
       "      <td>2853</td>\n",
       "      <td>53730</td>\n",
       "      <td>19</td>\n",
       "      <td>trm_550b53d7dd674</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>motor fMRI task paradigm</td>\n",
       "      <td>None / Other</td>\n",
       "      <td>2853-53730_spmT_0003_1.nii.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>30085317</td>\n",
       "      <td>6123525.0</td>\n",
       "      <td>10.1093/scan/nsy064</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2978</td>\n",
       "      <td>Functional Connectivity in the Social Brain ac...</td>\n",
       "      <td>neurovault</td>\n",
       "      <td>ICA Component 2: Ventral Stream</td>\n",
       "      <td>Z</td>\n",
       "      <td>images/2978/component0010.nii.gz</td>\n",
       "      <td>2978</td>\n",
       "      <td>54071</td>\n",
       "      <td>50</td>\n",
       "      <td>trm_4f2453ce33f16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>social judgment task</td>\n",
       "      <td>None / Other</td>\n",
       "      <td>2978-54071_component0010.nii.gz</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>305 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          pmid       pmcid                               doi secondary_doi  \\\n",
       "3317  38066069  10709616.0                               NaN           NaN   \n",
       "1218  29069521   5716095.0               10.1093/scan/nsx125           NaN   \n",
       "1163  29753107         NaN  10.1016/j.neuroimage.2018.05.025           NaN   \n",
       "1165  25062683         NaN    10.1016/j.biopsych.2014.06.005           NaN   \n",
       "1001  29437891   5858598.0                               NaN           NaN   \n",
       "...        ...         ...                               ...           ...   \n",
       "2601  34147817   8220377.0        10.1016/j.nicl.2021.102723           NaN   \n",
       "3198  37713673         NaN              10.1162/jocn_a_02055           NaN   \n",
       "174   27989844         NaN  10.1016/j.neuroimage.2016.10.041           NaN   \n",
       "831   28957344   5619738.0      10.1371/journal.pone.0185152           NaN   \n",
       "906   30085317   6123525.0               10.1093/scan/nsy064           NaN   \n",
       "\n",
       "      collection_id                                    collection_name  \\\n",
       "3317          13520  Experience sampling reveals the role that cove...   \n",
       "1218           3952  Love flows downstream: mothers’ and children’s...   \n",
       "1163           3814  The neural basis of free language choice in bi...   \n",
       "1165           3655  Corticostriatal Control of Goal-Directed Actio...   \n",
       "1001           3554  Abstract memory representations in the ventrom...   \n",
       "...             ...                                                ...   \n",
       "2601           8993  Whole-brain functional correlates of memory fo...   \n",
       "3198          13693  Neural Mechanisms Underlying Trust to Friends,...   \n",
       "174            1848  In need of constraint: Understanding the role ...   \n",
       "831            2853  Functional hemispheric asymmetries during the ...   \n",
       "906            2978  Functional Connectivity in the Social Brain ac...   \n",
       "\n",
       "          source                                         image_name map_type  \\\n",
       "3317      pubmed                                         Non-target        Z   \n",
       "1218  neurovault  children's brain self-family contrast negative...        Z   \n",
       "1163  neurovault                                          Figure 2a        T   \n",
       "1165  neurovault  Valued actions simple effect in 18 healthy adults        T   \n",
       "1001      pubmed                         Prototype model correlates        Z   \n",
       "...          ...                                                ...      ...   \n",
       "2601  neurovault  across group subsequentmemory scenes participa...        T   \n",
       "3198  neurovault  Positive T-test Whole brain Regression with Ge...        T   \n",
       "174   neurovault        Seed_i9_Perseverance_NegativeContrast_zstat        Z   \n",
       "831   neurovault               Planning lateralization right hand S        T   \n",
       "906   neurovault                    ICA Component 2: Ventral Stream        Z   \n",
       "\n",
       "                                             image_file  collection_id.1  \\\n",
       "3317                images/13520/nontarget_group.nii.gz            13520   \n",
       "1218                          images/3952/zstat8.nii.gz             3952   \n",
       "1163                       images/3814/spmT_0002.nii.gz             3814   \n",
       "1165                       images/3655/spmT_0002.nii.gz             3655   \n",
       "1001                          images/3554/zstat1.nii.gz             3554   \n",
       "...                                                 ...              ...   \n",
       "2601  images/8993/across-group_subsequentmemory-scen...             8993   \n",
       "3198                    images/13693/spmT_0001_6.nii.gz            13693   \n",
       "174                        images/1848/zstat13_4.nii.gz             1848   \n",
       "831                      images/2853/spmT_0003_1.nii.gz             2853   \n",
       "906                    images/2978/component0010.nii.gz             2978   \n",
       "\n",
       "      image_id  number_of_subjects cognitive_paradigm_cogatlas_id  \\\n",
       "3317    798383                  57              trm_4da86cfe8cf1b   \n",
       "1218     65066                  22              trm_4da890594742a   \n",
       "1163     64145                  15              tsk_4a57abb949cfb   \n",
       "1165     62601                  18              trm_4f2414059baa8   \n",
       "1001     61824                  29              trm_4f24112057e90   \n",
       "...        ...                 ...                            ...   \n",
       "2601    459316                  60              tsk_mFS3uwUMAhXxe   \n",
       "3198    795325                  92              tsk_Ncknr0soiM4IV   \n",
       "174      28822                 204              trm_56a91a92043bc   \n",
       "831      53730                  19              trm_550b53d7dd674   \n",
       "906      54071                  50              trm_4f2453ce33f16   \n",
       "\n",
       "     cognitive_contrast_cogatlas_id  \\\n",
       "3317                          Other   \n",
       "1218              cnt_553a6ea7469af   \n",
       "1163              cnt_4df6a77053b3f   \n",
       "1165                            NaN   \n",
       "1001              cnt_55a3034ca0943   \n",
       "...                             ...   \n",
       "2601                            NaN   \n",
       "3198                            NaN   \n",
       "174                             NaN   \n",
       "831                             NaN   \n",
       "906                             NaN   \n",
       "\n",
       "                                    contrast_definition  \\\n",
       "3317                                                NaN   \n",
       "1218  mothers' attention to self pain > their child ...   \n",
       "1163  naming in English (L2) versus naming in German...   \n",
       "1165     Simple effect of valued actions delta function   \n",
       "1001  prototype model predictor versus implicit base...   \n",
       "...                                                 ...   \n",
       "2601                                      across-groups   \n",
       "3198                                                NaN   \n",
       "174                                                 NaN   \n",
       "831                                                 NaN   \n",
       "906                                                 NaN   \n",
       "\n",
       "          cognitive_paradigm_cogatlas_name  \\\n",
       "3317  sustained attention to response task   \n",
       "1218             emotional regulation task   \n",
       "1163                   picture naming task   \n",
       "1165            instrumental learning task   \n",
       "1001                   categorization task   \n",
       "...                                    ...   \n",
       "2601                  Memory encoding task   \n",
       "3198           social decision-making task   \n",
       "174               UPPS-P Impulsivity Scale   \n",
       "831               motor fMRI task paradigm   \n",
       "906                   social judgment task   \n",
       "\n",
       "                       cognitive_contrast_cogatlas_name  \\\n",
       "3317                                              Other   \n",
       "1218                                                all   \n",
       "1163  accuracy of participant minus average accuracy...   \n",
       "1165                                       None / Other   \n",
       "1001                                    percent correct   \n",
       "...                                                 ...   \n",
       "2601                                       None / Other   \n",
       "3198                                       None / Other   \n",
       "174                                        None / Other   \n",
       "831                                        None / Other   \n",
       "906                                        None / Other   \n",
       "\n",
       "                                             image_path  \n",
       "3317                13520-798383_nontarget_group.nii.gz  \n",
       "1218                           3952-65066_zstat8.nii.gz  \n",
       "1163                        3814-64145_spmT_0002.nii.gz  \n",
       "1165                        3655-62601_spmT_0002.nii.gz  \n",
       "1001                           3554-61824_zstat1.nii.gz  \n",
       "...                                                 ...  \n",
       "2601  8993-459316_across-group_subsequentmemory-scen...  \n",
       "3198                    13693-795325_spmT_0001_6.nii.gz  \n",
       "174                         1848-28822_zstat13_4.nii.gz  \n",
       "831                       2853-53730_spmT_0003_1.nii.gz  \n",
       "906                     2978-54071_component0010.nii.gz  \n",
       "\n",
       "[305 rows x 19 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coll_image_selected_df = pd.concat([coll_one_image_df, image_selected_df])\n",
    "coll_image_selected_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_selected_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mimage_selected_df\u001b[49m\u001b[38;5;241m.\u001b[39mto_csv(op\u001b[38;5;241m.\u001b[39mjoin(data_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnv_collections_images_selected.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m), index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'image_selected_df' is not defined"
     ]
    }
   ],
   "source": [
    "image_selected_df.to_csv(op.join(data_dir, \"nv_collections_images_selected.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_fn = \"/Users/jperaza/Documents/GitHub/NiMARE/nimare/resources/templates/MNI152_2x2x2_brainmask.nii.gz\"\n",
    "masker = get_masker(mask_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of duplicated images: same map, different stats\n",
    "img1_fn = \"/Users/jperaza/Documents/GitHub/large-scale-ibma/data/nv-data/images/13137-790833_speechRev_spmT_0001.nii.gz\"\n",
    "img2_fn = \"/Users/jperaza/Documents/GitHub/large-scale-ibma/data/nv-data/images/13137-790848_speechRev_Zmap.nii.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1.0000007, 0.9965031], dtype=float32),\n",
       " array([0.9965031, 1.0000005], dtype=float32)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_get_corr_matrix([img1_fn, img2_fn], masker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of duplicated images: same contrast, sign inverted\n",
    "img1_fn = (\n",
    "    \"/Users/jperaza/Documents/GitHub/large-scale-ibma/data/nv-data/images/3192-57498_zstat1.nii.gz\"\n",
    ")\n",
    "img2_fn = \"/Users/jperaza/Documents/GitHub/large-scale-ibma/data/nv-data/images/3192-57499_zstat1_1.nii.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.9999992, -0.9999992], dtype=float32),\n",
       " array([-0.9999992,  0.9999992], dtype=float32)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_get_corr_matrix([img1_fn, img2_fn], masker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should we drop image that have ICA or PCA in the file name?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
